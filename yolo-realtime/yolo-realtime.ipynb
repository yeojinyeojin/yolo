{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 28,
   "source": [
    "from __future__ import division\n",
    "\n",
    "import os\n",
    "import time\n",
    "\n",
    "import cv2\n",
    "import numpy as np"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# IMAGE WITH YOLOv3"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "source": [
    "LABELSPATH = '../../darknet/data/coco.names'\n",
    "CONFIGPATH = '../../darknet/cfg/yolov3.cfg'\n",
    "WEIGHTSPATH = '../../darknet/yolov3.weights'\n",
    "\n",
    "CONFIDENCE_THS = 0.5\n",
    "NMS_THS = 0.3"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "source": [
    "LABELS = open(LABELSPATH).read().strip().split(\"\\n\")\n",
    "\n",
    "np.random.seed(42)\n",
    "COLORS = np.random.randint(0,255,size=(len(LABELS),3),dtype=\"uint8\")\n",
    "\n",
    "net = cv2.dnn.readNetFromDarknet(CONFIGPATH,WEIGHTSPATH)\n",
    "net.setPreferableBackend(cv2.dnn.DNN_BACKEND_CUDA)\n",
    "net.setPreferableTarget(cv2.dnn.DNN_TARGET_CUDA)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "source": [
    "def test_on_image(net,image):\n",
    "    (H,W) = image.shape[:2]\n",
    "\n",
    "    output_ln = net.getLayerNames()\n",
    "    output_ln = [output_ln[i[0]-1] for i in net.getUnconnectedOutLayers()]\n",
    "\n",
    "    blob = cv2.dnn.blobFromImage(image,1/255.0,(416,416),swapRB=True,crop=False)\n",
    "    net.setInput(blob)\n",
    "    start = time.time()\n",
    "    layer_outputs = net.forward(output_ln)\n",
    "    end = time.time()\n",
    "\n",
    "    # output storages\n",
    "    boxes = []\n",
    "    confidences = []\n",
    "    classIDs = []\n",
    "\n",
    "    for output in layer_outputs:\n",
    "        for detection in output:\n",
    "            scores = detection[5:]\n",
    "            classID = np.argmax(scores)\n",
    "            confidence = scores[classID]\n",
    "\n",
    "            # filter out weak predictions\n",
    "            if confidence > CONFIDENCE_THS:\n",
    "                # scale the bounding box coordinates back relative to the size of the image\n",
    "                box = detection[0:4] * np.array([W,H,W,H])\n",
    "                (centerX,centerY,width,height) = box.astype(\"int\")\n",
    "\n",
    "                # derive the top left corner of the bounding box\n",
    "                x = int(centerX-(width/2))\n",
    "                y = int(centerY-(height/2))\n",
    "\n",
    "                # update our lists\n",
    "                boxes.append([x,y,int(width),int(height)])\n",
    "                confidences.append(float(confidence))\n",
    "                classIDs.append(classID)\n",
    "    \n",
    "    idxs = cv2.dnn.NMSBoxes(boxes,confidences,CONFIDENCE_THS,NMS_THS)\n",
    "\n",
    "    return idxs,boxes,confidences,classIDs,end-start"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "source": [
    "def draw_outputs(idxs,boxes,confidences,classIDs,image):\n",
    "    if len(idxs) > 0:\n",
    "        for i in idxs.flatten():\n",
    "            (x,y) = (boxes[i][0],boxes[i][1])\n",
    "            (w,h) = (boxes[i][2],boxes[i][3])\n",
    "\n",
    "            color = [int(c) for c in COLORS[classIDs[i]]]\n",
    "            cv2.rectangle(image,(x,y),(x+w,y+h),color,2)\n",
    "            text = \"{}:{:.4f}\".format(LABELS[classIDs[i]],confidences[i])\n",
    "            cv2.putText(image,text,(x,y-5),cv2.FONT_HERSHEY_SIMPLEX,0.5,color,2)\n",
    "    \n",
    "    return image"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "source": [
    "imagefile = '../images/dog-cycle-car.png'\n",
    "image = cv2.imread(imagefile)\n",
    "idxs,boxes,confidences,classIDs,duration = test_on_image(net,image)\n",
    "print(\"[INFO] YOLO took {:.6f} secs\".format(duration))\n",
    "output_image = draw_outputs(idxs,boxes,confidences,classIDs,image)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[INFO] YOLO took 0.364851 secs\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "source": [
    "cv2.imshow(\"\",output_image)\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()\n",
    "cv2.waitKey(1)"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "-1"
      ]
     },
     "metadata": {},
     "execution_count": 34
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# WEBCAM WITH YOLOv3"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "source": [
    "LABELSPATH = '../../darknet/data/coco.names'\n",
    "CONFIGPATH = '../../darknet/cfg/yolov3.cfg'\n",
    "WEIGHTSPATH = '../../darknet/yolov3.weights'\n",
    "\n",
    "CONFIDENCE_THS = 0.5\n",
    "NMS_THS = 0.3"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "source": [
    "LABELS = open(LABELSPATH).read().strip().split(\"\\n\")\n",
    "\n",
    "np.random.seed(42)\n",
    "COLORS = np.random.randint(0,255,size=(len(LABELS),3),dtype=\"uint8\")\n",
    "\n",
    "net = cv2.dnn.readNetFromDarknet(CONFIGPATH,WEIGHTSPATH)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "source": [
    "def test_on_webcam():\n",
    "    cam = cv2.VideoCapture(-1) #0=front-cam, 1=back-cam\n",
    "    cam.set(cv2.CAP_PROP_FRAME_WIDTH,1300)\n",
    "    cam.set(cv2.CAP_PROP_FRAME_HEIGHT,1500)\n",
    "\n",
    "    # counter and storage to compute average prediction time\n",
    "    cnt = 0\n",
    "    sum_durations = 0\n",
    "\n",
    "    if cam.read() == False:\n",
    "        cam.open()\n",
    "\n",
    "    if not cam.isOpened():\n",
    "        raise IOError(\"cannot open webcam\")\n",
    "\n",
    "    while True:\n",
    "        # start = time.time()\n",
    "        ret,frame = cam.read()\n",
    "\n",
    "        if not ret:\n",
    "            raise IOError(\"cannot receive frame\")\n",
    "        \n",
    "        idxs,boxes,confidences,classIDs,duration = test_on_image(net,frame)\n",
    "        sum_durations += duration\n",
    "        cnt += 1\n",
    "        \n",
    "        if len(idxs) > 0 :\n",
    "            for i in idxs.flatten():\n",
    "                (x,y) = (boxes[i][0],boxes[i][1])\n",
    "                (w,h) = (boxes[i][2],boxes[i][3])\n",
    "\n",
    "                # draw bounding box and label on the frame\n",
    "                color = [int(c) for c in COLORS[classIDs[i]]]\n",
    "                text = \"{}: {:.4f}\".format(LABELS[classIDs[i]],confidences[i])\n",
    "                frame = cv2.rectangle(frame,(x,y),(x+w,y+h),color,2)\n",
    "                frame = cv2.putText(frame,text,(x,y-5),cv2.FONT_HERSHEY_SIMPLEX,0.5,color,2)\n",
    "\n",
    "            \n",
    "        cv2.imshow('',frame)\n",
    "        if cv2.waitKey(1) == ord('q'):\n",
    "            break\n",
    "\n",
    "    cam.release()\n",
    "    cv2.destroyAllWindows()\n",
    "\n",
    "    print(\"[INFO] YOLO took {:.6f} secs per frame in average\".format(sum_durations/cnt))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "source": [
    "test_on_webcam()"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[INFO] YOLO took 0.285066 secs per frame in average\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# WEBCAM WITH YOLOv4\n",
    "\n",
    "## Major Improvements\n",
    "- BoF (bag of freebies) improve the accuracy of the detector without increasing the inference time (they only increase the training cost)\n",
    "- BoS (bag of specials) improves the accuracy of object detection while increasing the inference cost by a small amount\n",
    "\n",
    "## Performance\n",
    "- mAP: 43.5% on COCO dataset (10% increase from YOLOv3)\n",
    "- real-time speed: 65 FPS on Tesla V100 (12% increase from YOLOv3)"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "source": [
    "LABELSPATH = '../../darknet-alex/data/coco.names'\n",
    "CONFIGPATH = '../../darknet-alex/cfg/yolov4.cfg'\n",
    "WEIGHTSPATH = '../../darknet-alex/yolov4.weights'\n",
    "\n",
    "CONFIDENCE_THS = 0.5\n",
    "NMS_THS = 0.3"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "source": [
    "test_on_webcam()"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[INFO] YOLO took 0.272537 secs per frame in average\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# WEBCAM WITH TINY-YOLOv4"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "source": [
    "LABELSPATH = '../../darknet-alex/data/coco.names'\n",
    "CONFIGPATH = '../../darknet-alex/cfg/yolov4-tiny.cfg'\n",
    "WEIGHTSPATH = '../../darknet-alex/yolov4-tiny.weights'\n",
    "\n",
    "CONFIDENCE_THS = 0.5\n",
    "NMS_THS = 0.3"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "source": [
    "test_on_webcam()"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[INFO] YOLO took 0.260241 secs per frame in average\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# WEBCAM WITH PP-YOLO\n",
    "- based on YOLOv3 in PaddleDetection\n",
    "- goal: relatively balanced effectiveness and efficiency that can be directly applied in actual application scenarios rather than proposing a new detection model\n",
    "\n",
    "## Major Improvements\n",
    "- replace Darknet53 backbone of YOLOv3 with ResNet backbone (significant increase in the FPS)\n",
    "- increase training batch size from 64 to 192 (as mini-batch size of 24 on 8 GPUs)\n",
    "\n",
    "## Performance\n",
    "- mAP: 45.2% on COCO dataset (14.6% relative improvement to YOLOv4)\n",
    "- real-time speed: 72.9 FPS on Tesla V100"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# YOLOv5\n",
    "\n",
    "## Major Improvements\n",
    "- PyTorch implementation rather than a fork from original Darknet\n",
    "- CSP backbone and PA-NET neck\n",
    "- mosaic data augmentation\n",
    "- auto learning bounding box anchors\n",
    "\n",
    "## Performances\n",
    "- FPS: 140 (with Tesla P100)\n",
    "- weights file: 27 MB (v4: 244 MB; 90% smaller than V4)"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "source": [
    "import torch\n",
    "\n",
    "model = torch.hub.load('ultralytics/yolov5','yolov5m')\n",
    "img = 'https://ultralytics.com/images/zidane.jpg'\n",
    "results = model(img)\n",
    "\n",
    "results.print()"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Using cache found in /home/funzin/.cache/torch/hub/ultralytics_yolov5_master\n",
      "YOLOv5 🚀 2021-8-25 torch 1.9.0+cu102 CPU\n",
      "\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Downloading https://github.com/ultralytics/yolov5/releases/download/v5.0/yolov5m.pt to /home/funzin/.cache/torch/hub/ultralytics_yolov5_master/yolov5m.pt...\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "100%|██████████| 41.1M/41.1M [00:03<00:00, 11.4MB/s]\n",
      "Fusing layers... \n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Model Summary: 308 layers, 21356877 parameters, 0 gradients\n",
      "Adding AutoShape... \n",
      "image 1/1: 720x1280 2 persons, 1 tie\n",
      "Speed: 1081.4ms pre-process, 246.4ms inference, 0.8ms NMS per image at shape (1, 3, 384, 640)\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "source": [
    "results.show()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "source": [
    "os.system(\"python ../../yolov5/detect.py --source 0\")"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "metadata": {},
     "execution_count": 66
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "CONFIGPATH = '../../yolov5/models/yolov5s.yaml'\n",
    "WEIGHTSPATH = '../../yolov5/yolov5s.pt'\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## NOTE: Architecture\n",
    "- backbone: extract feature map from image\n",
    "    - v3: Darknet53\n",
    "    - v4, v5: CSP-Darknet\n",
    "- head: locate object based on the extracted feature map\n",
    "    - initialize anchor box (default box) and create final bounding box\n",
    "    - 3 scales: 8 pixel, 16, 32\n",
    "    - 3 anchor boxes/scale"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python",
   "version": "3.8.0",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.0 64-bit ('yoloing': conda)"
  },
  "interpreter": {
   "hash": "8816270150bc4a5ff6b744185fa9ee8005e66bbce302098329e884586c1533d5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}